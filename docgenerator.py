"""Command line interface for DocGen-LM.

This script scans a source tree for Python, MATLAB, C++, and Java files,
parses them, requests summaries from a running LLM, and writes HTML
documentation.

Examples
--------
Generate documentation for ``./project`` into ``./docs`` while ignoring
``tests`` and ``build`` directories::

    python docgenerator.py ./project --output ./docs --ignore tests --ignore build
"""

from __future__ import annotations

import argparse
import os
import shutil
import sys
from pathlib import Path
from typing import Any

from cache import ResponseCache
from html_writer import write_index, write_module_page
from llm_client import LLMClient, sanitize_summary, SYSTEM_PROMPT, PROMPT_TEMPLATES
from chunk_utils import get_tokenizer, chunk_text
from summarize_utils import summarize_chunked
from tqdm import tqdm
from parser_python import parse_python_file
from parser_matlab import parse_matlab_file
from parser_cpp import parse_cpp_file
from parser_java import parse_java_file
from scanner import scan_directory, _is_subpath


def clean_output_dir(output_dir: str) -> None:
    for filename in os.listdir(output_dir):
        if filename.endswith(".html"):
            full_path = os.path.join(output_dir, filename)
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    first_line = f.readline()
                    if "Generated by DocGen-LM" in first_line:
                        os.remove(full_path)
            except Exception as e:
                print(f"[WARNING] Could not check {filename}: {e}")

def _summarize(client: LLMClient, cache: ResponseCache, key: str, text: str, prompt_type: str) -> str:
    cached = cache.get(key)
    if cached is not None:
        return cached
    summary = client.summarize(text, prompt_type)
    cache.set(key, summary)
    return summary


def _chunk_module_by_structure(module: dict, tokenizer, chunk_size_tokens: int):
    """Return a list of text chunks for ``module`` using its parsed structure."""

    blocks = []
    module_doc = module.get("module_docstring")
    if module_doc:
        blocks.append(module_doc)

    for cls in module.get("classes", []):
        src = cls.get("source", "")
        if len(tokenizer.encode(src)) <= chunk_size_tokens:
            blocks.append(src)
        else:
            for method in cls.get("methods", []):
                m_src = method.get("source", "")
                blocks.append(m_src)
            for var in cls.get("variables", []):
                v_src = var.get("source", var.get("name", ""))
                blocks.append(v_src)

    for func in module.get("functions", []):
        blocks.append(func.get("source", func.get("signature", "")))

    chunks = []
    current: list[str] = []
    current_tokens = 0
    sep_tokens = len(tokenizer.encode("\n\n"))

    for block in blocks:
        block_tokens = len(tokenizer.encode(block))
        if block_tokens > chunk_size_tokens:
            if current:
                chunks.append("\n\n".join(current))
                current = []
                current_tokens = 0
            chunks.extend(chunk_text(block, tokenizer, chunk_size_tokens))
            continue

        additional = block_tokens if not current else block_tokens + sep_tokens
        if current_tokens + additional <= chunk_size_tokens:
            current.append(block)
            current_tokens += additional
        else:
            if current:
                chunks.append("\n\n".join(current))
            current = [block]
            current_tokens = block_tokens

    if current:
        chunks.append("\n\n".join(current))

    return chunks


# Backwards compatibility shim: import the shared helper
_summarize_chunked = summarize_chunked


def _summarize_module_chunked(
    client: LLMClient,
    cache: ResponseCache,
    key_prefix: str,
    module_text: str,
    module: dict,
    tokenizer,
    max_context_tokens: int,
    chunk_token_budget: int,
) -> str:
    """Summarize a module using structure-aware chunking."""

    template = PROMPT_TEMPLATES["module"]
    overhead_tokens = len(tokenizer.encode(SYSTEM_PROMPT)) + len(
        tokenizer.encode(template.format(text=""))
    )
    available_tokens = max(1, max_context_tokens - overhead_tokens)

    if len(tokenizer.encode(module_text)) <= available_tokens:
        key = ResponseCache.make_key(key_prefix, module_text)
        return _summarize(client, cache, key, module_text, "module")

    chunk_size_tokens = min(chunk_token_budget, available_tokens)
    try:
        parts = _chunk_module_by_structure(module, tokenizer, chunk_size_tokens)
    except Exception as exc:  # pragma: no cover - defensive
        print(f"[WARN] Structure-based chunking failed: {exc}", file=sys.stderr)
        key = ResponseCache.make_key(key_prefix, module_text)
        try:
            return _summarize(client, cache, key, module_text, "module")
        except Exception:
            return sanitize_summary("")

    partials = []
    for idx, part in enumerate(
        tqdm(parts, desc="Summarizing chunks", leave=False)
    ):
        key = ResponseCache.make_key(f"{key_prefix}:part{idx}", part)
        try:
            partials.append(_summarize(client, cache, key, part, "module"))
        except Exception as exc:  # pragma: no cover - network failure
            print(f"[WARN] Summarization failed for chunk {idx}: {exc}", file=sys.stderr)
    if not partials:
        return sanitize_summary("")

    instructions = (
        "You are a documentation generator.\n\n"
        "Combine the following summaries into a single technical paragraph.\n"
        "Do not critique, evaluate, or offer suggestions.\n"
        "Do not speculate or use uncertain language.\n"
        "Only summarize what the code explicitly defines.\n\n"
    )
    instr_tokens = len(tokenizer.encode(instructions))
    merge_budget = max(1, available_tokens - instr_tokens)

    def _merge_recursive(items: list[str], depth: int = 0) -> str:
        merge_text = "\n".join(f"- {p}" for p in items)
        prompt = instructions + merge_text
        if len(tokenizer.encode(prompt)) <= available_tokens:
            key = ResponseCache.make_key(f"{key_prefix}:merge{depth}", prompt)
            return _summarize(client, cache, key, prompt, "docstring")
        if len(items) == 1:
            single = items[0]
            key = ResponseCache.make_key(f"{key_prefix}:merge{depth}:solo", single)
            return _summarize_chunked(
                client,
                cache,
                key,
                single,
                "docstring",
                max_context_tokens=max_context_tokens,
                chunk_token_budget=chunk_token_budget,
            )
        groups: list[list[str]] = []
        current: list[str] = []
        current_tokens = 0
        for p in items:
            bullet = f"- {p}\n"
            b_tokens = len(tokenizer.encode(bullet))
            if current and current_tokens + b_tokens > merge_budget:
                groups.append(current)
                current = [p]
                current_tokens = b_tokens
            else:
                current.append(p)
                current_tokens += b_tokens
        if current:
            groups.append(current)

        merged: list[str] = []
        for idx, grp in enumerate(groups):
            merged.append(_merge_recursive(grp, depth + 1))
        return _merge_recursive(merged, depth + 1)

    try:
        final_summary = _merge_recursive(partials)
    except Exception as exc:  # pragma: no cover - network failure
        print(f"[WARN] Merge failed: {exc}", file=sys.stderr)
        return sanitize_summary("\n".join(partials))
    return sanitize_summary(final_summary)


DOC_PROMPT = (
    "You are a documentation engine.\n\n"
    "Generate a technical summary of the function or class below.\n"
    "- Do not include suggestions or conversational language.\n"
    "- Do not say \"this function\", \"you can\", or \"the following code\".\n"
    "- Do not refer to the instructions or docstring.\n"
    "- Just describe what the code implements, in 1–3 concise sentences.\n\n"
    "Code:\n```python\n"
    "{source}\n"
    "Docstring (optional):\n"
    '\"\"\"{docstring}\"\"\"\n'
    "```"
)

# Template for generating class-level summaries after methods are known
CLASS_PROMPT = (
    "You are summarizing the class `{class_name}`, which is part of a project that {project_summary}.\n\n"
    "This class defines the following methods:\n{methods}\n\n"
    "It also defines these public variables:\n{variables}\n\n"
    "Based on this structure, generate a concise summary (1–3 sentences) of what this class does.\n\n"
    "Do not include usage instructions. Do not mention unrelated domains like text-based games. "
    "Stick to the provided methods and context."
)


def _build_function_prompt(
    source: str,
    class_name: str | None = None,
    class_summary: str | None = None,
    project_summary: str | None = None,
) -> str:
    """Return a context-enriched prompt for summarizing ``source``."""

    lines = ["You are a documentation generator."]
    if class_name:
        lines.append(f"The following function is part of the class `{class_name}`.")
    if class_summary:
        lines.append(f"This class {class_summary}")
    if project_summary:
        lines.append(f"This project {project_summary}")
    lines.extend(
        [
            "Summarize the function based on its source code below.",
            "- Do not include assistant phrasing or usage instructions.",
            "- Do not mention unrelated games or systems.",
            "- Only use the context and code provided.",
            "",
            "```python",
            source,
            "```",
        ]
    )
    return "\n".join(lines)


def _rewrite_docstring(
    client: LLMClient,
    cache: ResponseCache,
    file_path: str,
    item: dict[str, str],
    *,
    class_name: str | None = None,
    class_summary: str | None = None,
    project_summary: str | None = None,
    tokenizer=None,
    max_context_tokens: int = 4096,
    chunk_token_budget: int = 3072,
) -> None:
    """Rewrite ``item`` docstring using optional context."""

    source = item.get("source", "")
    docstring = item.get("docstring", "") or ""
    if not source and not docstring:
        print(
            f"Warning: no source or docstring for {file_path}:{item.get('name')}",
            file=sys.stderr,
        )
        return

    if not docstring:
        return

    if class_name or class_summary or project_summary:
        prompt = _build_function_prompt(
            source,
            class_name=class_name,
            class_summary=class_summary,
            project_summary=project_summary,
        )
        key_content = source + (class_name or "") + (class_summary or "") + (project_summary or "")
    else:
        prompt = DOC_PROMPT.format(source=source, docstring=docstring)
        key_content = source + docstring

    key = ResponseCache.make_key(
        f"REWRITE:{file_path}:{item.get('name')}",
        key_content,
    )
    result = _summarize_chunked(
        client,
        cache,
        key,
        prompt,
        "docstring",
        max_context_tokens=max_context_tokens,
        chunk_token_budget=chunk_token_budget,
    )
    item["docstring"] = sanitize_summary(result) or "No summary available."


def _summarize_members_recursive(
    class_data: dict[str, Any],
    path: str,
    client: LLMClient,
    cache: ResponseCache,
    tokenizer,
    max_context_tokens: int,
    chunk_token_budget: int,
) -> None:
    """Summarize methods and variables of ``class_data`` and any subclasses."""

    for method in tqdm(class_data.get("methods", []), desc="methods", leave=False):
        src = method.get("source") or method.get("signature") or method.get("name", "")
        key = ResponseCache.make_key(
            f"{path}:{class_data.get('name')}:{method.get('name')}", src
        )
        summary = _summarize_chunked(
            client,
            cache,
            key,
            src,
            "function",
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )
        method["summary"] = summary
        method["docstring"] = summary

    for var in tqdm(class_data.get("variables", []), desc="variables", leave=False):
        src = var.get("source") or var.get("name", "")
        key = ResponseCache.make_key(
            f"{path}:{class_data.get('name')}:{var.get('name')}", src
        )
        summary = _summarize_chunked(
            client,
            cache,
            key,
            src,
            "function",
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )
        var["summary"] = summary
        var["docstring"] = summary

    for sub in class_data.get("subclasses", []):
        _summarize_members_recursive(
            sub,
            path,
            client,
            cache,
            tokenizer,
            max_context_tokens,
            chunk_token_budget,
        )


def _summarize_class_recursive(
    class_data: dict[str, Any],
    path: str,
    project_summary: str,
    tokenizer,
    client: LLMClient,
    cache: ResponseCache,
    max_context_tokens: int,
    chunk_token_budget: int,
) -> None:
    """Summarize ``class_data`` and rewrite its docstring and methods."""

    _summarize_members_recursive(
        class_data,
        path,
        client,
        cache,
        tokenizer,
        max_context_tokens,
        chunk_token_budget,
    )

    method_lines = []
    for method in class_data.get("methods", []):
        summary = method.get("summary", "")
        name = method.get("name", "")
        method_lines.append(f"- {name}: {summary}" if summary else f"- {name}")

    methods_text = "\n".join(method_lines) if method_lines else "- (no methods)"

    var_lines = []
    for var in class_data.get("variables", []):
        summary = var.get("summary", var.get("docstring", ""))
        name = var.get("name", "")
        var_lines.append(f"- {name}: {summary}" if summary else f"- {name}")

    variables_text = "\n".join(var_lines) if var_lines else "- (no variables)"

    class_prompt = CLASS_PROMPT.format(
        class_name=class_data.get("name", ""),
        project_summary=project_summary,
        methods=methods_text,
        variables=variables_text,
    )
    cls_key = ResponseCache.make_key(f"{path}:{class_data.get('name')}", class_prompt)
    cls_summary = _summarize_chunked(
        client,
        cache,
        cls_key,
        class_prompt,
        "docstring",
        max_context_tokens=max_context_tokens,
        chunk_token_budget=chunk_token_budget,
    )
    cls_summary = sanitize_summary(cls_summary)
    original_doc = class_data.get("docstring", "")
    class_data["summary"] = cls_summary
    class_data["docstring"] = cls_summary
    if original_doc:
        class_data["docstring"] = original_doc
        _rewrite_docstring(
            client,
            cache,
            path,
            class_data,
            tokenizer=tokenizer,
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )

    for method in class_data.get("methods", []):
        _rewrite_docstring(
            client,
            cache,
            path,
            method,
            class_name=class_data.get("name"),
            class_summary=cls_summary,
            project_summary=project_summary,
            tokenizer=tokenizer,
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )

    for var in class_data.get("variables", []):
        _rewrite_docstring(
            client,
            cache,
            path,
            var,
            class_name=class_data.get("name"),
            class_summary=cls_summary,
            project_summary=project_summary,
            tokenizer=tokenizer,
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )

    for sub in class_data.get("subclasses", []):
        _summarize_class_recursive(
            sub,
            path,
            project_summary,
            tokenizer,
            client,
            cache,
            max_context_tokens,
            chunk_token_budget,
        )


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="Generate HTML documentation using a local LLM")
    parser.add_argument("source", help="Path to the source directory")
    parser.add_argument("--output", required=True, help="Destination directory for HTML output")
    parser.add_argument(
        "--ignore",
        action="append",
        default=[],
        help="Paths relative to source that should be ignored (repeatable)",
    )
    parser.add_argument(
        "--llm-url",
        default="http://localhost:1234",
        help="Base URL of the LLM API",
    )
    parser.add_argument(
        "--model",
        default="local",
        help="Model name to use when contacting the LLM",
    )
    parser.add_argument(
        "--max-context-tokens",
        type=int,
        default=4096,
        help="Maximum token context window for the LLM",
    )
    parser.add_argument(
        "--clear-progress",
        action="store_true",
        help="Clear saved progress after a successful run",
    )
    args = parser.parse_args(argv)

    client = LLMClient(base_url=args.llm_url, model=args.model)
    try:
        client.ping()
    except ConnectionError as exc:
        print(str(exc), file=sys.stderr)
        return 1

    tokenizer = get_tokenizer()
    max_context_tokens = args.max_context_tokens
    chunk_token_budget = int(max_context_tokens * 0.75)

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    clean_output_dir(str(output_dir))
    static_dir = Path(__file__).parent / "static"
    # use absolute path so execution works from any current working directory
    shutil.copytree(static_dir, output_dir / "static", dirs_exist_ok=True)

    cache = ResponseCache(str(output_dir / "cache.json"))
    progress = cache.get_progress()
    processed_paths = set(progress.keys())

    files = scan_directory(args.source, args.ignore)
    modules = []
    for path in tqdm(files, desc="Processing modules"):
        if path in processed_paths:
            modules.append(progress[path])
            continue
        try:
            text = Path(path).read_text(encoding="utf-8")
        except UnicodeDecodeError as exc:  # skip files with invalid encoding
            print(f"Skipping {path}: {exc}", file=sys.stderr)
            continue

        try:
            if path.endswith(".py"):
                parsed = parse_python_file(path)
                language = "python"
            elif path.endswith((".cpp", ".h")):
                parsed = parse_cpp_file(path)
                language = "cpp"
            elif path.endswith(".java"):
                parsed = parse_java_file(path)
                language = "java"
            else:
                parsed = parse_matlab_file(path)
                language = "matlab"
        except SyntaxError as exc:  # malformed file should be ignored
            print(f"Skipping {path}: {exc}", file=sys.stderr)
            continue

        key = ResponseCache.make_key(path, text)
        summary = _summarize_module_chunked(
            client,
            cache,
            key,
            text,
            parsed,
            tokenizer,
            max_context_tokens,
            chunk_token_budget,
        )

        module = {
            "name": Path(path).stem,
            "language": language,
            "summary": summary,
            "filename": Path(path).name,
            "path": path,
        }
        module.update(parsed)

        # summarize methods now so class summaries can reference them later
        for cls in tqdm(module.get("classes", []), desc=f"{module['name']}: classes", leave=False):
            _summarize_members_recursive(
                cls,
                path,
                client,
                cache,
                tokenizer,
                max_context_tokens,
                chunk_token_budget,
            )

        # and for standalone functions (summarized later with project context)
        for func in tqdm(module.get("functions", []), desc="methods", leave=False):
            func["summary"] = ""

        modules.append(module)

    page_links = [(m["name"], f"{m['name']}.html") for m in modules]

    project_lines = ["Project structure:"]
    for mod in modules:
        project_lines.append(f"- {mod['filename']}")
        classes = mod.get("classes", []) or []
        functions = mod.get("functions", []) or []

        if not classes and not functions:
            print(
                f"Warning: {mod['filename']} has no classes or functions",
                file=sys.stderr,
            )
            project_lines.append("  - (no classes or functions defined)")
            continue

        for cls in classes:
            project_lines.append(f"  - Class: {cls.get('name', '')}")
            method_names = [m.get("name", "") for m in cls.get("methods", [])]
            methods_text = ", ".join(method_names) if method_names else "(none)"
            project_lines.append(f"    - Methods: {methods_text}")

        if functions:
            func_names = ", ".join(f.get("name", "") for f in functions)
            project_lines.append(f"  - Functions: {func_names}")

    project_outline = "\n".join(project_lines)

    # gather markdown documentation
    md_files: list[Path] = []
    readme = Path(args.source) / "README.md"
    if readme.exists():
        md_files.append(readme)

    base = Path(args.source).resolve()
    ignore_paths = {(base / p).resolve() for p in args.ignore}

    for root, dirs, _files in os.walk(base, topdown=True, followlinks=False):
        root_path = Path(root)

        # prune symlinks and ignored directories
        pruned: list[str] = []
        for d in dirs:
            dir_path = root_path / d
            if dir_path.is_symlink():
                continue
            if any(_is_subpath((dir_path).resolve(), ig) for ig in ignore_paths):
                continue
            pruned.append(d)
        dirs[:] = pruned

        docs_dirs = [d for d in dirs if d.lower() == "docs"]
        for d in docs_dirs:
            docs_path = root_path / d
            for r, d2, f2 in os.walk(docs_path, topdown=True, followlinks=False):
                r_path = Path(r)
                d2[:] = [
                    dd
                    for dd in d2
                    if not (r_path / dd).is_symlink()
                    and not any(_is_subpath((r_path / dd).resolve(), ig) for ig in ignore_paths)
                ]
                for name in f2:
                    if name.endswith(".md"):
                        md_files.append(Path(r_path) / name)

        # prevent os.walk from recursing into docs directories again
        dirs[:] = [d for d in dirs if d.lower() != "docs"]

    md_parts = []
    for md_file in md_files:
        try:
            md_parts.append(md_file.read_text(encoding='utf-8'))
        except Exception as exc:  # pragma: no cover - filesystem edge case
            print(f"Skipping {md_file}: {exc}", file=sys.stderr)

    md_context = "\n".join(md_parts).strip()
    readme_summary = ""
    if md_context:
        readme_key = ResponseCache.make_key("README", md_context)
        readme_summary = _summarize_chunked(
            client,
            cache,
            readme_key,
            md_context,
            "readme",
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )
        readme_summary = sanitize_summary(readme_summary)

    project_key = ResponseCache.make_key("PROJECT", project_outline)
    raw_summary = _summarize_chunked(
        client,
        cache,
        project_key,
        project_outline,
        "project",
        max_context_tokens=max_context_tokens,
        chunk_token_budget=chunk_token_budget,
    )
    project_summary = sanitize_summary(raw_summary)
    if readme_summary:
        project_summary = f"{readme_summary}\n{project_summary}".strip()

    # Now that the project summary is available, generate class and function summaries
    # and rewrite method/function docstrings with context.
    for module in modules:
        path = module.get("path", "")
        if path in processed_paths:
            continue
        for cls in tqdm(module.get("classes", []), desc=f"{module['name']}: classes", leave=False):
            _summarize_class_recursive(
                cls,
                path,
                project_summary,
                tokenizer,
                client,
                cache,
                max_context_tokens,
                chunk_token_budget,
            )
        for func in tqdm(module.get("functions", []), desc="methods", leave=False):
            src = func.get("source") or func.get("signature") or func.get("name", "")
            prompt = _build_function_prompt(src, project_summary=project_summary)
            func_key = ResponseCache.make_key(f"{path}:{func.get('name')}", prompt)
            func_summary = _summarize_chunked(
                client,
                cache,
                func_key,
                prompt,
                "docstring",
                max_context_tokens=max_context_tokens,
                chunk_token_budget=chunk_token_budget,
            )
            func["summary"] = func_summary
            _rewrite_docstring(
                client,
                cache,
                path,
                func,
                project_summary=project_summary,
                tokenizer=tokenizer,
                max_context_tokens=max_context_tokens,
                chunk_token_budget=chunk_token_budget,
            )

    module_summaries = {m["name"]: m.get("summary", "") for m in modules}
    write_index(str(output_dir), project_summary, page_links, module_summaries)
    for module in modules:
        write_module_page(str(output_dir), module, page_links)
        cache.mark_done(module.get("path", ""), module)
        processed_paths.add(module.get("path", ""))

    if args.clear_progress or len(processed_paths) == len(files):
        cache.clear_progress()

    return 0


if __name__ == "__main__":  # pragma: no cover - CLI entry
    raise SystemExit(main())
