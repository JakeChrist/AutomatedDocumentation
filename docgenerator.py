"""Command line interface for DocGen-LM.

This script scans a source tree for Python and MATLAB files, parses them,
requests summaries from a running LLM, and writes HTML documentation.

Examples
--------
Generate documentation for ``./project`` into ``./docs`` while ignoring
``tests`` and ``build`` directories::

    python docgenerator.py ./project --output ./docs --ignore tests --ignore build
"""

from __future__ import annotations

import argparse
import os
import shutil
import sys
from pathlib import Path
from typing import Any

from cache import ResponseCache
from html_writer import write_index, write_module_page
from llm_client import LLMClient, sanitize_summary, SYSTEM_PROMPT, PROMPT_TEMPLATES

try:  # optional dependency used for token counting
    import tiktoken
except Exception:  # pragma: no cover - optional import
    tiktoken = None
try:
    from tqdm import tqdm
except ImportError:  # pragma: no cover - optional import
    tqdm = lambda x, **kwargs: x
from parser_python import parse_python_file
from parser_matlab import parse_matlab_file
from scanner import scan_directory


def clean_output_dir(output_dir: str) -> None:
    for filename in os.listdir(output_dir):
        if filename.endswith(".html"):
            full_path = os.path.join(output_dir, filename)
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    first_line = f.readline()
                    if "Generated by DocGen-LM" in first_line:
                        os.remove(full_path)
            except Exception as e:
                print(f"[WARNING] Could not check {filename}: {e}")

def _summarize(client: LLMClient, cache: ResponseCache, key: str, text: str, prompt_type: str) -> str:
    cached = cache.get(key)
    if cached is not None:
        return cached
    summary = client.summarize(text, prompt_type)
    cache.set(key, summary)
    return summary


def _get_tokenizer():
    """Return a tokenizer object used for estimating token counts."""

    if tiktoken is not None:  # pragma: no cover - optional branch
        try:
            return tiktoken.get_encoding("cl100k_base")
        except Exception:  # pragma: no cover - fallback if model unknown
            return tiktoken.encoding_for_model("gpt-3.5-turbo")

    print(
        "[WARNING] tiktoken is not installed; token counts will be approximate.",
        file=sys.stderr,
    )

    class _Simple:
        def encode(self, text: str):
            return text.split()

        def decode(self, tokens):
            return " ".join(tokens)

    return _Simple()


def chunk_text(text: str, tokenizer, chunk_size_tokens: int):
    """Split ``text`` into chunks roughly ``chunk_size_tokens`` each."""

    tokens = tokenizer.encode(text)
    chunks = []
    for i in range(0, len(tokens), chunk_size_tokens):
        chunk = tokens[i : i + chunk_size_tokens]
        chunks.append(tokenizer.decode(chunk))
    return chunks


def _chunk_module_by_structure(module: dict, tokenizer, chunk_size_tokens: int):
    """Return a list of text chunks for ``module`` using its parsed structure."""

    blocks = []
    module_doc = module.get("module_docstring")
    if module_doc:
        blocks.append(module_doc)

    for cls in module.get("classes", []):
        src = cls.get("source", "")
        if len(tokenizer.encode(src)) <= chunk_size_tokens:
            blocks.append(src)
        else:
            for method in cls.get("methods", []):
                m_src = method.get("source", "")
                blocks.append(m_src)

    for func in module.get("functions", []):
        blocks.append(func.get("source", func.get("signature", "")))

    chunks = []
    current: list[str] = []
    current_tokens = 0
    sep_tokens = len(tokenizer.encode("\n\n"))

    for block in blocks:
        block_tokens = len(tokenizer.encode(block))
        if block_tokens > chunk_size_tokens:
            if current:
                chunks.append("\n\n".join(current))
                current = []
                current_tokens = 0
            chunks.extend(chunk_text(block, tokenizer, chunk_size_tokens))
            continue

        additional = block_tokens if not current else block_tokens + sep_tokens
        if current_tokens + additional <= chunk_size_tokens:
            current.append(block)
            current_tokens += additional
        else:
            if current:
                chunks.append("\n\n".join(current))
            current = [block]
            current_tokens = block_tokens

    if current:
        chunks.append("\n\n".join(current))

    return chunks


def _summarize_chunked(
    client: LLMClient,
    cache: ResponseCache,
    key_prefix: str,
    text: str,
    prompt_type: str,
    tokenizer,
    max_context_tokens: int,
    chunk_token_budget: int,
) -> str:
    """Summarize ``text`` by chunking if necessary."""

    template = PROMPT_TEMPLATES.get(prompt_type, PROMPT_TEMPLATES["module"])
    overhead_tokens = len(tokenizer.encode(SYSTEM_PROMPT)) + len(
        tokenizer.encode(template.format(text=""))
    )
    available_tokens = max(1, max_context_tokens - overhead_tokens)

    if len(tokenizer.encode(text)) <= available_tokens:
        key = ResponseCache.make_key(key_prefix, text)
        return _summarize(client, cache, key, text, prompt_type)

    chunk_size_tokens = min(chunk_token_budget, available_tokens)
    parts = chunk_text(text, tokenizer, chunk_size_tokens)
    partials = []
    for idx, part in enumerate(parts):
        key = ResponseCache.make_key(f"{key_prefix}:part{idx}", part)
        partials.append(_summarize(client, cache, key, part, prompt_type))

    merge_text = "\n".join(f"- {p}" for p in partials)
    merge_prompt = (
        "You are a documentation generator.\n\n"
        "Combine the following summaries into a single technical paragraph.\n"
        "Do not critique, evaluate, or offer suggestions.\n"
        "Do not speculate or use uncertain language.\n"
        "Only summarize what the code explicitly defines.\n\n"
        + merge_text
    )
    merge_key = ResponseCache.make_key(f"{key_prefix}:merge", merge_prompt)
    final_summary = _summarize(client, cache, merge_key, merge_prompt, "docstring")
    return sanitize_summary(final_summary)


def _summarize_module_chunked(
    client: LLMClient,
    cache: ResponseCache,
    key_prefix: str,
    module_text: str,
    module: dict,
    tokenizer,
    max_context_tokens: int,
    chunk_token_budget: int,
) -> str:
    """Summarize a module using structure-aware chunking."""

    template = PROMPT_TEMPLATES["module"]
    overhead_tokens = len(tokenizer.encode(SYSTEM_PROMPT)) + len(
        tokenizer.encode(template.format(text=""))
    )
    available_tokens = max(1, max_context_tokens - overhead_tokens)

    if len(tokenizer.encode(module_text)) <= available_tokens:
        key = ResponseCache.make_key(key_prefix, module_text)
        return _summarize(client, cache, key, module_text, "module")

    chunk_size_tokens = min(chunk_token_budget, available_tokens)
    parts = _chunk_module_by_structure(module, tokenizer, chunk_size_tokens)
    partials = []
    for idx, part in enumerate(parts):
        key = ResponseCache.make_key(f"{key_prefix}:part{idx}", part)
        partials.append(_summarize(client, cache, key, part, "module"))

    merge_text = "\n".join(f"- {p}" for p in partials)
    merge_prompt = (
        "You are a documentation generator.\n\n"
        "Combine the following summaries into a single technical paragraph.\n"
        "Do not critique, evaluate, or offer suggestions.\n"
        "Do not speculate or use uncertain language.\n"
        "Only summarize what the code explicitly defines.\n\n"
        + merge_text
    )
    merge_key = ResponseCache.make_key(f"{key_prefix}:merge", merge_prompt)
    final_summary = _summarize(client, cache, merge_key, merge_prompt, "docstring")
    return sanitize_summary(final_summary)


DOC_PROMPT = (
    "You are a documentation engine.\n\n"
    "Generate a technical summary of the function or class below.\n"
    "- Do not include suggestions or conversational language.\n"
    "- Do not say \"this function\", \"you can\", or \"the following code\".\n"
    "- Do not refer to the instructions or docstring.\n"
    "- Just describe what the code implements, in 1–3 concise sentences.\n\n"
    "Code:\n```python\n"
    "{source}\n"
    "Docstring (optional):\n"
    '\"\"\"{docstring}\"\"\"\n'
    "```"
)

# Template for generating class-level summaries after methods are known
CLASS_PROMPT = (
    "You are summarizing the class `{class_name}`, which is part of a project that {project_summary}.\n\n"
    "This class defines the following methods:\n{methods}\n\n"
    "Based on this structure, generate a concise summary (1–3 sentences) of what this class does.\n\n"
    "Do not include usage instructions. Do not mention unrelated domains like text-based games. "
    "Stick to the provided methods and context."
)


def _build_function_prompt(
    source: str,
    class_name: str | None = None,
    class_summary: str | None = None,
    project_summary: str | None = None,
) -> str:
    """Return a context-enriched prompt for summarizing ``source``."""

    lines = ["You are a documentation generator."]
    if class_name:
        lines.append(f"The following function is part of the class `{class_name}`.")
    if class_summary:
        lines.append(f"This class {class_summary}")
    if project_summary:
        lines.append(f"This project {project_summary}")
    lines.extend(
        [
            "Summarize the function based on its source code below.",
            "- Do not include assistant phrasing or usage instructions.",
            "- Do not mention unrelated games or systems.",
            "- Only use the context and code provided.",
            "",
            "```python",
            source,
            "```",
        ]
    )
    return "\n".join(lines)


def _rewrite_docstring(
    client: LLMClient,
    cache: ResponseCache,
    file_path: str,
    item: dict[str, str],
    *,
    class_name: str | None = None,
    class_summary: str | None = None,
    project_summary: str | None = None,
    tokenizer=None,
    max_context_tokens: int = 4096,
    chunk_token_budget: int = 3072,
) -> None:
    """Rewrite ``item`` docstring using optional context."""

    source = item.get("source", "")
    docstring = item.get("docstring", "") or ""
    if not source and not docstring:
        print(
            f"Warning: no source or docstring for {file_path}:{item.get('name')}",
            file=sys.stderr,
        )
        return

    if not docstring:
        return

    if class_name or class_summary or project_summary:
        prompt = _build_function_prompt(
            source,
            class_name=class_name,
            class_summary=class_summary,
            project_summary=project_summary,
        )
        key_content = source + (class_name or "") + (class_summary or "") + (project_summary or "")
    else:
        prompt = DOC_PROMPT.format(source=source, docstring=docstring)
        key_content = source + docstring

    key = ResponseCache.make_key(
        f"REWRITE:{file_path}:{item.get('name')}",
        key_content,
    )
    result = _summarize_chunked(
        client,
        cache,
        key,
        prompt,
        "docstring",
        tokenizer,
        max_context_tokens,
        chunk_token_budget,
    )
    item["docstring"] = sanitize_summary(result) or "No summary available."


def _summarize_methods_recursive(
    class_data: dict[str, Any],
    path: str,
    client: LLMClient,
    cache: ResponseCache,
    tokenizer,
    max_context_tokens: int,
    chunk_token_budget: int,
) -> None:
    """Summarize methods of ``class_data`` and any subclasses."""

    for method in tqdm(class_data.get("methods", []), desc="methods", leave=False):
        src = method.get("source") or method.get("signature") or method.get("name", "")
        key = ResponseCache.make_key(
            f"{path}:{class_data.get('name')}:{method.get('name')}", src
        )
        summary = _summarize_chunked(
            client,
            cache,
            key,
            src,
            "function",
            tokenizer,
            max_context_tokens,
            chunk_token_budget,
        )
        method["summary"] = summary
        method["docstring"] = summary

    for sub in class_data.get("subclasses", []):
        _summarize_methods_recursive(
            sub,
            path,
            client,
            cache,
            tokenizer,
            max_context_tokens,
            chunk_token_budget,
        )


def _summarize_class_recursive(
    class_data: dict[str, Any],
    path: str,
    project_summary: str,
    tokenizer,
    client: LLMClient,
    cache: ResponseCache,
    max_context_tokens: int,
    chunk_token_budget: int,
) -> None:
    """Summarize ``class_data`` and rewrite its docstring and methods."""

    _summarize_methods_recursive(
        class_data,
        path,
        client,
        cache,
        tokenizer,
        max_context_tokens,
        chunk_token_budget,
    )

    method_lines = []
    for method in class_data.get("methods", []):
        summary = method.get("summary", "")
        name = method.get("name", "")
        method_lines.append(f"- {name}: {summary}" if summary else f"- {name}")

    methods_text = "\n".join(method_lines) if method_lines else "- (no methods)"
    class_prompt = CLASS_PROMPT.format(
        class_name=class_data.get("name", ""),
        project_summary=project_summary,
        methods=methods_text,
    )
    cls_key = ResponseCache.make_key(f"{path}:{class_data.get('name')}", class_prompt)
    cls_summary = _summarize_chunked(
        client,
        cache,
        cls_key,
        class_prompt,
        "docstring",
        tokenizer,
        max_context_tokens,
        chunk_token_budget,
    )
    cls_summary = sanitize_summary(cls_summary)
    original_doc = class_data.get("docstring", "")
    class_data["summary"] = cls_summary
    class_data["docstring"] = cls_summary
    if original_doc:
        class_data["docstring"] = original_doc
        _rewrite_docstring(
            client,
            cache,
            path,
            class_data,
            tokenizer=tokenizer,
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )

    for method in class_data.get("methods", []):
        _rewrite_docstring(
            client,
            cache,
            path,
            method,
            class_name=class_data.get("name"),
            class_summary=cls_summary,
            project_summary=project_summary,
            tokenizer=tokenizer,
            max_context_tokens=max_context_tokens,
            chunk_token_budget=chunk_token_budget,
        )

    for sub in class_data.get("subclasses", []):
        _summarize_class_recursive(
            sub,
            path,
            project_summary,
            tokenizer,
            client,
            cache,
            max_context_tokens,
            chunk_token_budget,
        )


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="Generate HTML documentation using a local LLM")
    parser.add_argument("source", help="Path to the source directory")
    parser.add_argument("--output", required=True, help="Destination directory for HTML output")
    parser.add_argument(
        "--ignore",
        action="append",
        default=[],
        help="Paths relative to source that should be ignored (repeatable)",
    )
    parser.add_argument(
        "--llm-url",
        default="http://localhost:1234",
        help="Base URL of the LLM API",
    )
    parser.add_argument(
        "--model",
        default="local",
        help="Model name to use when contacting the LLM",
    )
    parser.add_argument(
        "--max-context-tokens",
        type=int,
        default=4096,
        help="Maximum token context window for the LLM",
    )
    args = parser.parse_args(argv)

    client = LLMClient(base_url=args.llm_url, model=args.model)
    try:
        client.ping()
    except ConnectionError as exc:
        print(str(exc), file=sys.stderr)
        return 1

    tokenizer = _get_tokenizer()
    max_context_tokens = args.max_context_tokens
    chunk_token_budget = int(max_context_tokens * 0.75)

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    clean_output_dir(str(output_dir))
    static_dir = Path(__file__).parent / "static"
    # use absolute path so execution works from any current working directory
    shutil.copytree(static_dir, output_dir / "static", dirs_exist_ok=True)

    cache = ResponseCache(str(output_dir / "cache.json"))

    files = scan_directory(args.source, args.ignore)
    modules = []
    for path in tqdm(files, desc="Processing modules"):
        try:
            text = Path(path).read_text(encoding="utf-8")
        except UnicodeDecodeError as exc:  # skip files with invalid encoding
            print(f"Skipping {path}: {exc}", file=sys.stderr)
            continue

        try:
            if path.endswith(".py"):
                parsed = parse_python_file(path)
                language = "python"
            else:
                parsed = parse_matlab_file(path)
                language = "matlab"
        except SyntaxError as exc:  # malformed file should be ignored
            print(f"Skipping {path}: {exc}", file=sys.stderr)
            continue

        key = ResponseCache.make_key(path, text)
        summary = _summarize_module_chunked(
            client,
            cache,
            key,
            text,
            parsed,
            tokenizer,
            max_context_tokens,
            chunk_token_budget,
        )

        module = {
            "name": Path(path).stem,
            "language": language,
            "summary": summary,
            "filename": Path(path).name,
            "path": path,
        }
        module.update(parsed)

        # summarize methods now so class summaries can reference them later
        for cls in tqdm(module.get("classes", []), desc=f"{module['name']}: classes", leave=False):
            _summarize_methods_recursive(
                cls,
                path,
                client,
                cache,
                tokenizer,
                max_context_tokens,
                chunk_token_budget,
            )

        # and for standalone functions (summarized later with project context)
        for func in tqdm(module.get("functions", []), desc="methods", leave=False):
            func["summary"] = ""

        modules.append(module)

    page_links = [(m["name"], f"{m['name']}.html") for m in modules]

    project_lines = ["Project structure:"]
    for mod in modules:
        project_lines.append(f"- {mod['filename']}")
        classes = mod.get("classes", []) or []
        functions = mod.get("functions", []) or []

        if not classes and not functions:
            print(
                f"Warning: {mod['filename']} has no classes or functions",
                file=sys.stderr,
            )
            project_lines.append("  - (no classes or functions defined)")
            continue

        for cls in classes:
            project_lines.append(f"  - Class: {cls.get('name', '')}")
            method_names = [m.get("name", "") for m in cls.get("methods", [])]
            methods_text = ", ".join(method_names) if method_names else "(none)"
            project_lines.append(f"    - Methods: {methods_text}")

        if functions:
            func_names = ", ".join(f.get("name", "") for f in functions)
            project_lines.append(f"  - Functions: {func_names}")

    project_outline = "\n".join(project_lines)

    # gather markdown documentation
    md_files = []
    readme = Path(args.source) / "README.md"
    if readme.exists():
        md_files.append(readme)
    for p in Path(args.source).rglob('*'):
        if p.is_dir() and p.name.lower() == 'docs':
            for md in p.rglob('*.md'):
                md_files.append(md)

    md_parts = []
    for md_file in md_files:
        try:
            md_parts.append(md_file.read_text(encoding='utf-8'))
        except Exception as exc:  # pragma: no cover - filesystem edge case
            print(f"Skipping {md_file}: {exc}", file=sys.stderr)

    md_context = "\n".join(md_parts).strip()
    readme_summary = ""
    if md_context:
        readme_key = ResponseCache.make_key("README", md_context)
        readme_summary = _summarize_chunked(
            client,
            cache,
            readme_key,
            md_context,
            "readme",
            tokenizer,
            max_context_tokens,
            chunk_token_budget,
        )
        readme_summary = sanitize_summary(readme_summary)

    project_key = ResponseCache.make_key("PROJECT", project_outline)
    raw_summary = _summarize_chunked(
        client,
        cache,
        project_key,
        project_outline,
        "project",
        tokenizer,
        max_context_tokens,
        chunk_token_budget,
    )
    project_summary = sanitize_summary(raw_summary)
    if readme_summary:
        project_summary = f"{readme_summary}\n{project_summary}".strip()

    # Now that the project summary is available, generate class and function summaries
    # and rewrite method/function docstrings with context.
    for module in modules:
        path = module.get("path", "")
        for cls in tqdm(module.get("classes", []), desc=f"{module['name']}: classes", leave=False):
            _summarize_class_recursive(
                cls,
                path,
                project_summary,
                tokenizer,
                client,
                cache,
                max_context_tokens,
                chunk_token_budget,
            )
        for func in tqdm(module.get("functions", []), desc="methods", leave=False):
            src = func.get("source") or func.get("signature") or func.get("name", "")
            prompt = _build_function_prompt(src, project_summary=project_summary)
            func_key = ResponseCache.make_key(f"{path}:{func.get('name')}", prompt)
            func_summary = _summarize_chunked(
                client,
                cache,
                func_key,
                prompt,
                "docstring",
                tokenizer,
                max_context_tokens,
                chunk_token_budget,
            )
            func["summary"] = func_summary
            _rewrite_docstring(
                client,
                cache,
                path,
                func,
                project_summary=project_summary,
                tokenizer=tokenizer,
                max_context_tokens=max_context_tokens,
                chunk_token_budget=chunk_token_budget,
            )

    module_summaries = {m["name"]: m.get("summary", "") for m in modules}
    write_index(str(output_dir), project_summary, page_links, module_summaries)
    for module in modules:
        write_module_page(str(output_dir), module, page_links)

    return 0


if __name__ == "__main__":  # pragma: no cover - CLI entry
    raise SystemExit(main())
