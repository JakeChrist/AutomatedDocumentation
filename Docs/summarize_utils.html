<!-- Generated by DocGen-LM -->
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
    
<title>summarize_utils</title>
<link href="static/style.css" rel="stylesheet"/>
</head>
<body class="doc-body">
<button aria-label="Toggle navigation menu" class="sidebar-toggle" id="sidebar-toggle">Menu</button>
<div aria-label="Documentation navigation" class="sidebar" role="navigation">
<h2>Navigation</h2>
<ul><li><a href="index.html"><strong>üè† Project Overview</strong></a></li><li><a href="cache.html">cache</a></li><li><a href="chunk_utils.html">chunk_utils</a></li><li><a href="docgenerator.html">docgenerator</a></li><li><a href="explaincode.html">explaincode</a></li><li><a href="gui_wrapper.html">gui_wrapper</a></li><li><a href="html_writer.html">html_writer</a></li><li><a href="llm_client.html">llm_client</a></li><li><a href="manual_utils.html">manual_utils</a></li><li><a href="parser_cpp.html">parser_cpp</a></li><li><a href="parser_java.html">parser_java</a></li><li><a href="parser_matlab.html">parser_matlab</a></li><li><a href="parser_python.html">parser_python</a></li><li><a href="retrofit_sidebar.html">retrofit_sidebar</a></li><li><a href="reviewer.html">reviewer</a></li><li><a href="sanitize_docs.html">sanitize_docs</a></li><li><a href="scanner.html">scanner</a></li><li><a href="setup.html">setup</a></li><li><a href="summarize_utils.html">summarize_utils</a></li><li><details><summary>tests</summary><ul><li><a href="test_cache.html">test_cache</a></li><li><a href="test_chunk_utils.html">test_chunk_utils</a></li><li><a href="test_docgenerator.html">test_docgenerator</a></li><li><a href="test_docgenerator_subclasses.html">test_docgenerator_subclasses</a></li><li><a href="test_explaincode.html">test_explaincode</a></li><li><a href="test_html_writer.html">test_html_writer</a></li><li><a href="test_integration.html">test_integration</a></li><li><a href="test_llm_client.html">test_llm_client</a></li><li><a href="test_manual_utils.html">test_manual_utils</a></li><li><a href="test_parser_cpp.html">test_parser_cpp</a></li><li><a href="test_parser_java.html">test_parser_java</a></li><li><a href="test_parser_matlab.html">test_parser_matlab</a></li><li><a href="test_parser_python.html">test_parser_python</a></li><li><a href="test_resume_progress.html">test_resume_progress</a></li><li><a href="test_retrofit_sidebar.html">test_retrofit_sidebar</a></li><li><a href="test_reviewer.html">test_reviewer</a></li><li><a href="test_sanitize_docs.html">test_sanitize_docs</a></li><li><a href="test_scanner.html">test_scanner</a></li></ul></details></li></ul>
</div>
<main class="content" role="main">
        <div class="content-inner">
<h1>summarize_utils</h1>
<p>Module implements a chunked text summarization function that uses an LLM client and response cache. It handles text that exceeds token limits by splitting into chunks, summarizing each chunk, and recursively merging summaries. The function manages token overhead from system prompts and templates, falls back to single-summary mode for short texts, and includes error handling for chunking, summarization, and merging steps. Cached responses are sanitized before being returned.</p>
<h2>Functions</h2>
<h3 id="_summarize">_summarize(client: LLMClient, cache: ResponseCache, key: str, text: str, prompt_type: str, *, system_prompt: str) -&gt; str</h3>
<p>The function `_summarize` retrieves or generates a summary for a given text using an LLM client and caches the result. It first checks if a cached response exists for the provided key. If so, it returns the cached summary after sanitizing it. Otherwise, it calls the client to generate a new summary, stores the result in the cache, and returns the generated summary. The function uses a system prompt to guide the summarization process and ensures that cached entries are sanitized before being returned.</p>
<pre><code><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">_summarize</span>(
    client: LLMClient,
    cache: ResponseCache,
    key: <span style="color: #008000">str</span>,
    text: <span style="color: #008000">str</span>,
    prompt_type: <span style="color: #008000">str</span>,
    <span style="color: #666666">*</span>,
    system_prompt: <span style="color: #008000">str</span>,
) <span style="color: #666666">-&gt;</span> <span style="color: #008000">str</span>:
    cached <span style="color: #666666">=</span> cache<span style="color: #666666">.</span>get(key)
    <span style="color: #008000; font-weight: bold">if</span> cached <span style="color: #AA22FF; font-weight: bold">is</span> <span style="color: #AA22FF; font-weight: bold">not</span> <span style="color: #008000; font-weight: bold">None</span>:
        <span style="color: #3D7B7B; font-style: italic"># Old cache entries might contain reserved tokens; sanitize to keep</span>
        <span style="color: #3D7B7B; font-style: italic"># later tokenization safe.</span>
        <span style="color: #008000; font-weight: bold">return</span> sanitize_summary(cached)
    summary <span style="color: #666666">=</span> client<span style="color: #666666">.</span>summarize(text, prompt_type, system_prompt<span style="color: #666666">=</span>system_prompt)
    cache<span style="color: #666666">.</span>set(key, summary)
    <span style="color: #008000; font-weight: bold">return</span> summary
</code></pre>
<h3 id="summarize_chunked">summarize_chunked(client: LLMClient, cache: ResponseCache, key_prefix: str, text: str, prompt_type: str, *, system_prompt: str=SYSTEM_PROMPT, max_context_tokens: int=4096, chunk_token_budget: int=3072) -&gt; str</h3>
<p>The function `summarize_chunked` summarizes input text by either processing it as a single chunk or breaking it into smaller parts if the text exceeds token limits. It uses an LLM client and cache for efficient processing, and employs tokenization to determine appropriate chunk sizes based on context and budget constraints. If the input text fits within the available token limit, it is directly summarized. Otherwise, the text is split into chunks, each of which is summarized individually. These partial summaries are then merged recursively using a merging prompt until a final consolidated summary is produced. In case of failures during chunking, summarization, or merging, the function falls back to returning sanitized text or an empty string. The function supports configurable system prompts and handles token overhead for system and template prompts. It includes error handling for network issues and tokenization errors, ensuring robust operation even when individual steps fail.</p>
<pre><code><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">summarize_chunked</span>(
    client: LLMClient,
    cache: ResponseCache,
    key_prefix: <span style="color: #008000">str</span>,
    text: <span style="color: #008000">str</span>,
    prompt_type: <span style="color: #008000">str</span>,
    <span style="color: #666666">*</span>,
    system_prompt: <span style="color: #008000">str</span> <span style="color: #666666">=</span> SYSTEM_PROMPT,
    max_context_tokens: <span style="color: #008000">int</span> <span style="color: #666666">=</span> <span style="color: #666666">4096</span>,
    chunk_token_budget: <span style="color: #008000">int</span> <span style="color: #666666">=</span> <span style="color: #666666">3072</span>,
) <span style="color: #666666">-&gt;</span> <span style="color: #008000">str</span>:
<span style="color: #bbbbbb">    </span><span style="color: #BA2121; font-style: italic">"""Summarize ``text`` by chunking if necessary."""</span>

    tokenizer <span style="color: #666666">=</span> get_tokenizer()
    max_context_tokens <span style="color: #666666">=</span> <span style="color: #008000">min</span>(max_context_tokens, MAX_CHUNK_TOKENS)
    chunk_token_budget <span style="color: #666666">=</span> <span style="color: #008000">min</span>(chunk_token_budget, MAX_CHUNK_TOKENS)
    template <span style="color: #666666">=</span> PROMPT_TEMPLATES<span style="color: #666666">.</span>get(prompt_type, PROMPT_TEMPLATES[<span style="color: #BA2121">"module"</span>])
    overhead_tokens <span style="color: #666666">=</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(system_prompt)) <span style="color: #666666">+</span> <span style="color: #008000">len</span>(
        tokenizer<span style="color: #666666">.</span>encode(template<span style="color: #666666">.</span>format(text<span style="color: #666666">=</span><span style="color: #BA2121">""</span>))
    )
    available_tokens <span style="color: #666666">=</span> <span style="color: #008000">max</span>(<span style="color: #666666">1</span>, max_context_tokens <span style="color: #666666">-</span> overhead_tokens)

    <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(text)) <span style="color: #666666">&lt;=</span> available_tokens:
        key <span style="color: #666666">=</span> ResponseCache<span style="color: #666666">.</span>make_key(key_prefix, text)
        <span style="color: #008000; font-weight: bold">return</span> _summarize(
            client, cache, key, text, prompt_type, system_prompt<span style="color: #666666">=</span>system_prompt
        )

    chunk_size_tokens <span style="color: #666666">=</span> <span style="color: #008000">min</span>(chunk_token_budget, available_tokens, MAX_CHUNK_TOKENS)
    <span style="color: #008000; font-weight: bold">try</span>:
        parts <span style="color: #666666">=</span> chunk_text(text, tokenizer, chunk_size_tokens)
    <span style="color: #008000; font-weight: bold">except</span> <span style="color: #CB3F38; font-weight: bold">Exception</span> <span style="color: #008000; font-weight: bold">as</span> exc:  <span style="color: #3D7B7B; font-style: italic"># pragma: no cover - defensive</span>
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f"[WARN] Chunking failed: </span><span style="color: #A45A77; font-weight: bold">{</span>exc<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>, file<span style="color: #666666">=</span>sys<span style="color: #666666">.</span>stderr)
        key <span style="color: #666666">=</span> ResponseCache<span style="color: #666666">.</span>make_key(key_prefix, text)
        <span style="color: #008000; font-weight: bold">try</span>:
            <span style="color: #008000; font-weight: bold">return</span> _summarize(
                client, cache, key, text, prompt_type, system_prompt<span style="color: #666666">=</span>system_prompt
            )
        <span style="color: #008000; font-weight: bold">except</span> <span style="color: #CB3F38; font-weight: bold">Exception</span>:
            <span style="color: #008000; font-weight: bold">return</span> sanitize_summary(<span style="color: #BA2121">""</span>)

    partials: List[<span style="color: #008000">str</span>] <span style="color: #666666">=</span> []
    <span style="color: #008000; font-weight: bold">for</span> idx, part <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(parts):
        key <span style="color: #666666">=</span> ResponseCache<span style="color: #666666">.</span>make_key(<span style="color: #BA2121">f"</span><span style="color: #A45A77; font-weight: bold">{</span>key_prefix<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">:part</span><span style="color: #A45A77; font-weight: bold">{</span>idx<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>, part)
        <span style="color: #008000; font-weight: bold">try</span>:
            partials<span style="color: #666666">.</span>append(
                _summarize(
                    client,
                    cache,
                    key,
                    part,
                    prompt_type,
                    system_prompt<span style="color: #666666">=</span>system_prompt,
                )
            )
        <span style="color: #008000; font-weight: bold">except</span> <span style="color: #CB3F38; font-weight: bold">Exception</span> <span style="color: #008000; font-weight: bold">as</span> exc:  <span style="color: #3D7B7B; font-style: italic"># pragma: no cover - network failure</span>
            <span style="color: #008000">print</span>(
                <span style="color: #BA2121">f"[WARN] Summarization failed for chunk </span><span style="color: #A45A77; font-weight: bold">{</span>idx<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">: </span><span style="color: #A45A77; font-weight: bold">{</span>exc<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>,
                file<span style="color: #666666">=</span>sys<span style="color: #666666">.</span>stderr,
            )
    <span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> partials:
        <span style="color: #008000; font-weight: bold">return</span> sanitize_summary(<span style="color: #BA2121">""</span>)

    instructions <span style="color: #666666">=</span> (
        <span style="color: #BA2121">"You are a documentation generator.</span><span style="color: #AA5D1F; font-weight: bold">\n\n</span><span style="color: #BA2121">"</span>
        <span style="color: #BA2121">"Combine the following summaries into a single technical paragraph.</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span>
        <span style="color: #BA2121">"Do not critique, evaluate, or offer suggestions.</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span>
        <span style="color: #BA2121">"Do not speculate or use uncertain language.</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span>
        <span style="color: #BA2121">"Only summarize what the text explicitly states.</span><span style="color: #AA5D1F; font-weight: bold">\n\n</span><span style="color: #BA2121">"</span>
    )
    instr_tokens <span style="color: #666666">=</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(instructions))
    merge_budget <span style="color: #666666">=</span> <span style="color: #008000">max</span>(<span style="color: #666666">1</span>, available_tokens <span style="color: #666666">-</span> instr_tokens)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">_merge_recursive</span>(items: List[<span style="color: #008000">str</span>], depth: <span style="color: #008000">int</span> <span style="color: #666666">=</span> <span style="color: #666666">0</span>) <span style="color: #666666">-&gt;</span> <span style="color: #008000">str</span>:
        merge_text <span style="color: #666666">=</span> <span style="color: #BA2121">"</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span><span style="color: #666666">.</span>join(<span style="color: #BA2121">f"- </span><span style="color: #A45A77; font-weight: bold">{</span>p<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span> <span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> items)
        prompt <span style="color: #666666">=</span> instructions <span style="color: #666666">+</span> merge_text
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(prompt)) <span style="color: #666666">&lt;=</span> available_tokens:
            key <span style="color: #666666">=</span> ResponseCache<span style="color: #666666">.</span>make_key(<span style="color: #BA2121">f"</span><span style="color: #A45A77; font-weight: bold">{</span>key_prefix<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">:merge</span><span style="color: #A45A77; font-weight: bold">{</span>depth<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>, prompt)
            <span style="color: #008000; font-weight: bold">return</span> _summarize(
                client,
                cache,
                key,
                prompt,
                <span style="color: #BA2121">"docstring"</span>,
                system_prompt<span style="color: #666666">=</span>system_prompt,
            )
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(items) <span style="color: #666666">==</span> <span style="color: #666666">1</span>:
            single <span style="color: #666666">=</span> items[<span style="color: #666666">0</span>]
            key <span style="color: #666666">=</span> ResponseCache<span style="color: #666666">.</span>make_key(<span style="color: #BA2121">f"</span><span style="color: #A45A77; font-weight: bold">{</span>key_prefix<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">:merge</span><span style="color: #A45A77; font-weight: bold">{</span>depth<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">:solo"</span>, single)
            <span style="color: #008000; font-weight: bold">return</span> summarize_chunked(
                client,
                cache,
                key,
                single,
                <span style="color: #BA2121">"docstring"</span>,
                system_prompt<span style="color: #666666">=</span>system_prompt,
                max_context_tokens<span style="color: #666666">=</span>max_context_tokens,
                chunk_token_budget<span style="color: #666666">=</span>chunk_token_budget,
            )
        groups: List[List[<span style="color: #008000">str</span>]] <span style="color: #666666">=</span> []
        current: List[<span style="color: #008000">str</span>] <span style="color: #666666">=</span> []
        current_tokens <span style="color: #666666">=</span> <span style="color: #666666">0</span>
        <span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> items:
            bullet <span style="color: #666666">=</span> <span style="color: #BA2121">f"- </span><span style="color: #A45A77; font-weight: bold">{</span>p<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span>
            b_tokens <span style="color: #666666">=</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(bullet))
            <span style="color: #008000; font-weight: bold">if</span> current <span style="color: #AA22FF; font-weight: bold">and</span> current_tokens <span style="color: #666666">+</span> b_tokens <span style="color: #666666">&gt;</span> merge_budget:
                groups<span style="color: #666666">.</span>append(current)
                current <span style="color: #666666">=</span> [p]
                current_tokens <span style="color: #666666">=</span> b_tokens
            <span style="color: #008000; font-weight: bold">else</span>:
                current<span style="color: #666666">.</span>append(p)
                current_tokens <span style="color: #666666">+=</span> b_tokens
        <span style="color: #008000; font-weight: bold">if</span> current:
            groups<span style="color: #666666">.</span>append(current)

        merged: List[<span style="color: #008000">str</span>] <span style="color: #666666">=</span> []
        <span style="color: #008000; font-weight: bold">for</span> idx, grp <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(groups):
            merged<span style="color: #666666">.</span>append(_merge_recursive(grp, depth <span style="color: #666666">+</span> <span style="color: #666666">1</span>))
        <span style="color: #008000; font-weight: bold">return</span> _merge_recursive(merged, depth <span style="color: #666666">+</span> <span style="color: #666666">1</span>)

    <span style="color: #008000; font-weight: bold">try</span>:
        final_summary <span style="color: #666666">=</span> _merge_recursive(partials)
    <span style="color: #008000; font-weight: bold">except</span> <span style="color: #CB3F38; font-weight: bold">Exception</span> <span style="color: #008000; font-weight: bold">as</span> exc:  <span style="color: #3D7B7B; font-style: italic"># pragma: no cover - network failure</span>
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f"[WARN] Merge failed: </span><span style="color: #A45A77; font-weight: bold">{</span>exc<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>, file<span style="color: #666666">=</span>sys<span style="color: #666666">.</span>stderr)
        <span style="color: #008000; font-weight: bold">return</span> sanitize_summary(<span style="color: #BA2121">"</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span><span style="color: #666666">.</span>join(partials))
    <span style="color: #008000; font-weight: bold">return</span> sanitize_summary(final_summary)
</code></pre>
<details>
<summary>Subfunction: _merge_recursive(items: List[str], depth: int=0) -&gt; str</summary>
<h4 id="_merge_recursive">_merge_recursive(items: List[str], depth: int=0) -&gt; str</h4>
<p>The function `summarize_chunked` processes large text inputs by recursively summarizing them using a language model. It handles input that exceeds token limits by splitting it into chunks, summarizing each chunk, and then merging the summaries. The merging process is recursive, combining summaries in a hierarchical manner to maintain coherence while respecting token constraints. If a merged prompt still exceeds the token limit, it further subdivides the input or applies recursive summarization to individual items. The function uses caching to avoid reprocessing identical prompts and supports configurable context and chunk size parameters for fine-tuning performance.</p>
<pre><code><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">_merge_recursive</span>(items: List[<span style="color: #008000">str</span>], depth: <span style="color: #008000">int</span> <span style="color: #666666">=</span> <span style="color: #666666">0</span>) <span style="color: #666666">-&gt;</span> <span style="color: #008000">str</span>:
        merge_text <span style="color: #666666">=</span> <span style="color: #BA2121">"</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span><span style="color: #666666">.</span>join(<span style="color: #BA2121">f"- </span><span style="color: #A45A77; font-weight: bold">{</span>p<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span> <span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> items)
        prompt <span style="color: #666666">=</span> instructions <span style="color: #666666">+</span> merge_text
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(prompt)) <span style="color: #666666">&lt;=</span> available_tokens:
            key <span style="color: #666666">=</span> ResponseCache<span style="color: #666666">.</span>make_key(<span style="color: #BA2121">f"</span><span style="color: #A45A77; font-weight: bold">{</span>key_prefix<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">:merge</span><span style="color: #A45A77; font-weight: bold">{</span>depth<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>, prompt)
            <span style="color: #008000; font-weight: bold">return</span> _summarize(
                client,
                cache,
                key,
                prompt,
                <span style="color: #BA2121">"docstring"</span>,
                system_prompt<span style="color: #666666">=</span>system_prompt,
            )
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(items) <span style="color: #666666">==</span> <span style="color: #666666">1</span>:
            single <span style="color: #666666">=</span> items[<span style="color: #666666">0</span>]
            key <span style="color: #666666">=</span> ResponseCache<span style="color: #666666">.</span>make_key(<span style="color: #BA2121">f"</span><span style="color: #A45A77; font-weight: bold">{</span>key_prefix<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">:merge</span><span style="color: #A45A77; font-weight: bold">{</span>depth<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">:solo"</span>, single)
            <span style="color: #008000; font-weight: bold">return</span> summarize_chunked(
                client,
                cache,
                key,
                single,
                <span style="color: #BA2121">"docstring"</span>,
                system_prompt<span style="color: #666666">=</span>system_prompt,
                max_context_tokens<span style="color: #666666">=</span>max_context_tokens,
                chunk_token_budget<span style="color: #666666">=</span>chunk_token_budget,
            )
        groups: List[List[<span style="color: #008000">str</span>]] <span style="color: #666666">=</span> []
        current: List[<span style="color: #008000">str</span>] <span style="color: #666666">=</span> []
        current_tokens <span style="color: #666666">=</span> <span style="color: #666666">0</span>
        <span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> items:
            bullet <span style="color: #666666">=</span> <span style="color: #BA2121">f"- </span><span style="color: #A45A77; font-weight: bold">{</span>p<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span>
            b_tokens <span style="color: #666666">=</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(bullet))
            <span style="color: #008000; font-weight: bold">if</span> current <span style="color: #AA22FF; font-weight: bold">and</span> current_tokens <span style="color: #666666">+</span> b_tokens <span style="color: #666666">&gt;</span> merge_budget:
                groups<span style="color: #666666">.</span>append(current)
                current <span style="color: #666666">=</span> [p]
                current_tokens <span style="color: #666666">=</span> b_tokens
            <span style="color: #008000; font-weight: bold">else</span>:
                current<span style="color: #666666">.</span>append(p)
                current_tokens <span style="color: #666666">+=</span> b_tokens
        <span style="color: #008000; font-weight: bold">if</span> current:
            groups<span style="color: #666666">.</span>append(current)

        merged: List[<span style="color: #008000">str</span>] <span style="color: #666666">=</span> []
        <span style="color: #008000; font-weight: bold">for</span> idx, grp <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(groups):
            merged<span style="color: #666666">.</span>append(_merge_recursive(grp, depth <span style="color: #666666">+</span> <span style="color: #666666">1</span>))
        <span style="color: #008000; font-weight: bold">return</span> _merge_recursive(merged, depth <span style="color: #666666">+</span> <span style="color: #666666">1</span>)
</code></pre>
</details>
</div>
    </main>
<script src="static/toggle.js"></script>
</body>
</html>
