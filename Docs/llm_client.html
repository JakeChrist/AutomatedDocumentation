<!-- Generated by DocGen-LM -->
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
    
<title>llm_client</title>
<link href="static/style.css" rel="stylesheet"/>
</head>
<body class="doc-body">
<button aria-label="Toggle navigation menu" class="sidebar-toggle" id="sidebar-toggle">Menu</button>
<div aria-label="Documentation navigation" class="sidebar" role="navigation">
<h2>Navigation</h2>
<ul><li><a href="index.html"><strong>üè† Project Overview</strong></a></li><li><a href="cache.html">cache</a></li><li><a href="chunk_utils.html">chunk_utils</a></li><li><a href="docgenerator.html">docgenerator</a></li><li><a href="explaincode.html">explaincode</a></li><li><a href="gui_wrapper.html">gui_wrapper</a></li><li><a href="html_writer.html">html_writer</a></li><li><a href="llm_client.html">llm_client</a></li><li><a href="manual_utils.html">manual_utils</a></li><li><a href="parser_cpp.html">parser_cpp</a></li><li><a href="parser_java.html">parser_java</a></li><li><a href="parser_matlab.html">parser_matlab</a></li><li><a href="parser_python.html">parser_python</a></li><li><a href="retrofit_sidebar.html">retrofit_sidebar</a></li><li><a href="reviewer.html">reviewer</a></li><li><a href="sanitize_docs.html">sanitize_docs</a></li><li><a href="scanner.html">scanner</a></li><li><a href="setup.html">setup</a></li><li><a href="summarize_utils.html">summarize_utils</a></li><li><details><summary>tests</summary><ul><li><a href="test_cache.html">test_cache</a></li><li><a href="test_chunk_utils.html">test_chunk_utils</a></li><li><a href="test_docgenerator.html">test_docgenerator</a></li><li><a href="test_docgenerator_subclasses.html">test_docgenerator_subclasses</a></li><li><a href="test_explaincode.html">test_explaincode</a></li><li><a href="test_html_writer.html">test_html_writer</a></li><li><a href="test_integration.html">test_integration</a></li><li><a href="test_llm_client.html">test_llm_client</a></li><li><a href="test_manual_utils.html">test_manual_utils</a></li><li><a href="test_parser_cpp.html">test_parser_cpp</a></li><li><a href="test_parser_java.html">test_parser_java</a></li><li><a href="test_parser_matlab.html">test_parser_matlab</a></li><li><a href="test_parser_python.html">test_parser_python</a></li><li><a href="test_resume_progress.html">test_resume_progress</a></li><li><a href="test_retrofit_sidebar.html">test_retrofit_sidebar</a></li><li><a href="test_reviewer.html">test_reviewer</a></li><li><a href="test_sanitize_docs.html">test_sanitize_docs</a></li><li><a href="test_scanner.html">test_scanner</a></li></ul></details></li></ul>
</div>
<main class="content" role="main">
        <div class="content-inner">
<h1>llm_client</h1>
<p>The module defines an interface for interacting with a local LLM backend, specifically designed for generating code summaries. It includes prompt templates for different code elements such as modules, classes, functions, and README files, along with a system prompt that instructs the model to produce factual, concise descriptions without self-reference or additional commentary. The `LLMClient` class handles communication with the LMStudio API, sending requests and processing responses while managing token counts and retry logic for failed requests. A `sanitize_summary` function cleans generated summaries by removing meta-commentary and prompt-related content that could cause issues downstream. The module uses HTTP requests to interact with the LLM backend and includes utilities for tokenization and FIM token stripping to ensure compatibility with the model's input requirements.</p>
<h2 id="LLMClient">Class: LLMClient</h2>
<p>The `LLMClient` class provides a wrapper for interacting with the LMStudio HTTP API, offering methods to check API availability and generate text summaries using configured prompts and models. The `ping` method verifies connectivity to the LMStudio server, raising a `ConnectionError` if the server is unreachable. The `summarize` method constructs a prompt from a template, checks token usage against a budget, sends a request to the API with retry logic, and returns a sanitized summary response.</p>
<h3 id="__init__">Method: __init__(self, base_url: str='http://localhost:1234', model: str='local') -&gt; None</h3>
<p>Initializes the `LLMClient` instance with a specified base URL and model name. Sets up the endpoint for API requests by appending the chat completions path to the base URL, ensuring no trailing slash is present. The model parameter specifies which language model to use for requests.</p>
<pre><code><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, base_url: <span style="color: #008000">str</span> <span style="color: #666666">=</span> <span style="color: #BA2121">"http://localhost:1234"</span>, model: <span style="color: #008000">str</span> <span style="color: #666666">=</span> <span style="color: #BA2121">"local"</span>) <span style="color: #666666">-&gt;</span> <span style="color: #008000; font-weight: bold">None</span>:
        <span style="color: #008000">self</span><span style="color: #666666">.</span>base_url <span style="color: #666666">=</span> base_url<span style="color: #666666">.</span>rstrip(<span style="color: #BA2121">"/"</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>endpoint <span style="color: #666666">=</span> <span style="color: #BA2121">f"</span><span style="color: #A45A77; font-weight: bold">{</span><span style="color: #008000">self</span><span style="color: #666666">.</span>base_url<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">/v1/chat/completions"</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>model <span style="color: #666666">=</span> model
</code></pre>
<h3 id="ping">Method: ping(self, timeout: float=2.0) -&gt; bool</h3>
<p>The function `ping` checks the connectivity of an API endpoint specified by `self.base_url`. It sends an HTTP GET request with a specified timeout and raises a `ConnectionError` if the request fails or the server is unreachable. If the request is successful, it returns `True`. The function uses `requests.get` and handles exceptions using `RequestException` to provide a clear error message indicating the failure to connect to the LMStudio server.</p>
<pre><code><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">ping</span>(<span style="color: #008000">self</span>, timeout: <span style="color: #008000">float</span> <span style="color: #666666">=</span> <span style="color: #666666">2.0</span>) <span style="color: #666666">-&gt;</span> <span style="color: #008000">bool</span>:
<span style="color: #bbbbbb">        </span><span style="color: #BA2121; font-style: italic">"""Return ``True`` if the API is reachable.</span>

<span style="color: #BA2121; font-style: italic">        Raises</span>
<span style="color: #BA2121; font-style: italic">        ------</span>
<span style="color: #BA2121; font-style: italic">        ConnectionError</span>
<span style="color: #BA2121; font-style: italic">            If the server cannot be contacted.</span>
<span style="color: #BA2121; font-style: italic">        """</span>

        <span style="color: #008000; font-weight: bold">try</span>:
            response <span style="color: #666666">=</span> requests<span style="color: #666666">.</span>get(<span style="color: #008000">self</span><span style="color: #666666">.</span>base_url, timeout<span style="color: #666666">=</span>timeout)
            response<span style="color: #666666">.</span>raise_for_status()
            <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000; font-weight: bold">True</span>
        <span style="color: #008000; font-weight: bold">except</span> RequestException <span style="color: #008000; font-weight: bold">as</span> exc:
            <span style="color: #008000; font-weight: bold">raise</span> <span style="color: #CB3F38; font-weight: bold">ConnectionError</span>(<span style="color: #BA2121">f"Unable to reach LMStudio at </span><span style="color: #A45A77; font-weight: bold">{</span><span style="color: #008000">self</span><span style="color: #666666">.</span>base_url<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>) <span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">exc</span>
</code></pre>
<h3 id="summarize">Method: summarize(self, text: str, prompt_type: str, system_prompt: str=SYSTEM_PROMPT, *, chunk_token_budget: int | None=None, max_tokens: int=256) -&gt; str</h3>
<p>The `summarize` method generates a summary of the provided text using a specified prompt template and communicates with a language model via an HTTP endpoint. It constructs a prompt based on the input text and a selected template, checks the token count against a budget, and sends a request to the LLM with system and user messages. The method includes retry logic for handling request failures, processes streaming response data, and returns a sanitized summary string. If all retries fail, it raises a `RuntimeError` with the last error message.</p>
<pre><code><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">summarize</span>(
        <span style="color: #008000">self</span>,
        text: <span style="color: #008000">str</span>,
        prompt_type: <span style="color: #008000">str</span>,
        system_prompt: <span style="color: #008000">str</span> <span style="color: #666666">=</span> SYSTEM_PROMPT,
        <span style="color: #666666">*</span>,
        chunk_token_budget: <span style="color: #008000">int</span> <span style="color: #666666">|</span> <span style="color: #008000; font-weight: bold">None</span> <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
        max_tokens: <span style="color: #008000">int</span> <span style="color: #666666">=</span> <span style="color: #666666">256</span>,
    ) <span style="color: #666666">-&gt;</span> <span style="color: #008000">str</span>:
<span style="color: #bbbbbb">        </span><span style="color: #BA2121; font-style: italic">"""Return a summary for ``text`` using ``prompt_type`` template.</span>

<span style="color: #BA2121; font-style: italic">        Parameters</span>
<span style="color: #BA2121; font-style: italic">        ----------</span>
<span style="color: #BA2121; font-style: italic">        text:</span>
<span style="color: #BA2121; font-style: italic">            Text to summarize.</span>
<span style="color: #BA2121; font-style: italic">        prompt_type:</span>
<span style="color: #BA2121; font-style: italic">            Key into :data:`PROMPT_TEMPLATES` controlling the prompt format.</span>
<span style="color: #BA2121; font-style: italic">        system_prompt:</span>
<span style="color: #BA2121; font-style: italic">            Optional system instructions prepended to the conversation.</span>
<span style="color: #BA2121; font-style: italic">        chunk_token_budget:</span>
<span style="color: #BA2121; font-style: italic">            Maximum allowed tokens for the prompt.  A warning is emitted if the</span>
<span style="color: #BA2121; font-style: italic">            prompt exceeds this value.</span>
<span style="color: #BA2121; font-style: italic">        max_tokens:</span>
<span style="color: #BA2121; font-style: italic">            Maximum number of tokens the model may generate in its response.</span>
<span style="color: #BA2121; font-style: italic">        """</span>

        template <span style="color: #666666">=</span> PROMPT_TEMPLATES<span style="color: #666666">.</span>get(prompt_type, PROMPT_TEMPLATES[<span style="color: #BA2121">"module"</span>])
        prompt <span style="color: #666666">=</span> template<span style="color: #666666">.</span>format(text<span style="color: #666666">=</span>text)

        tokenizer <span style="color: #666666">=</span> get_tokenizer()
        prompt_tokens <span style="color: #666666">=</span> <span style="color: #008000">len</span>(tokenizer<span style="color: #666666">.</span>encode(prompt)) <span style="color: #666666">+</span> <span style="color: #008000">len</span>(
            tokenizer<span style="color: #666666">.</span>encode(system_prompt)
        )
        prompt_chars <span style="color: #666666">=</span> <span style="color: #008000">len</span>(prompt) <span style="color: #666666">+</span> <span style="color: #008000">len</span>(system_prompt)
        logging<span style="color: #666666">.</span>info(
            <span style="color: #BA2121">"Prompt size: </span><span style="color: #A45A77; font-weight: bold">%d</span><span style="color: #BA2121"> tokens, </span><span style="color: #A45A77; font-weight: bold">%d</span><span style="color: #BA2121"> chars"</span>, prompt_tokens, prompt_chars
        )
        <span style="color: #008000; font-weight: bold">if</span> chunk_token_budget <span style="color: #AA22FF; font-weight: bold">is</span> <span style="color: #AA22FF; font-weight: bold">not</span> <span style="color: #008000; font-weight: bold">None</span> <span style="color: #AA22FF; font-weight: bold">and</span> prompt_tokens <span style="color: #666666">&gt;</span> chunk_token_budget:
            logging<span style="color: #666666">.</span>warning(
                <span style="color: #BA2121">"Prompt tokens </span><span style="color: #A45A77; font-weight: bold">%d</span><span style="color: #BA2121"> exceed chunk_token_budget </span><span style="color: #A45A77; font-weight: bold">%d</span><span style="color: #BA2121">"</span>,
                prompt_tokens,
                chunk_token_budget,
            )

        payload: Dict[<span style="color: #008000">str</span>, Any] <span style="color: #666666">=</span> {
            <span style="color: #BA2121">"model"</span>: <span style="color: #008000">self</span><span style="color: #666666">.</span>model,
            <span style="color: #BA2121">"temperature"</span>: <span style="color: #666666">0.3</span>,
            <span style="color: #BA2121">"max_tokens"</span>: max_tokens,
            <span style="color: #BA2121">"messages"</span>: [
                {<span style="color: #BA2121">"role"</span>: <span style="color: #BA2121">"system"</span>, <span style="color: #BA2121">"content"</span>: system_prompt},
                {<span style="color: #BA2121">"role"</span>: <span style="color: #BA2121">"user"</span>, <span style="color: #BA2121">"content"</span>: prompt},
            ],
        }

        error_message <span style="color: #666666">=</span> <span style="color: #BA2121">""</span>
        response <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>
        <span style="color: #008000; font-weight: bold">for</span> _ <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">3</span>):
            <span style="color: #008000; font-weight: bold">try</span>:
                logging<span style="color: #666666">.</span>info(<span style="color: #BA2121">"LLM request started"</span>)
                response <span style="color: #666666">=</span> requests<span style="color: #666666">.</span>post(
                    <span style="color: #008000">self</span><span style="color: #666666">.</span>endpoint, json<span style="color: #666666">=</span>payload, timeout<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>, stream<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>
                )
                response<span style="color: #666666">.</span>raise_for_status()

                content_bytes <span style="color: #666666">=</span> <span style="color: #008000">bytearray</span>()
                last_log <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()
                <span style="color: #008000; font-weight: bold">try</span>:
                    <span style="color: #008000; font-weight: bold">for</span> chunk <span style="color: #AA22FF; font-weight: bold">in</span> response<span style="color: #666666">.</span>iter_content(chunk_size<span style="color: #666666">=8192</span>):
                        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">isinstance</span>(chunk, (<span style="color: #008000">bytes</span>, <span style="color: #008000">bytearray</span>)):
                            content_bytes<span style="color: #666666">.</span>extend(chunk)
                            logging<span style="color: #666666">.</span>debug(<span style="color: #BA2121">"Received </span><span style="color: #A45A77; font-weight: bold">%d</span><span style="color: #BA2121"> bytes"</span>, <span style="color: #008000">len</span>(chunk))
                        <span style="color: #008000; font-weight: bold">else</span>:  <span style="color: #3D7B7B; font-style: italic"># pragma: no cover - non-bytes from mock objects</span>
                            <span style="color: #008000; font-weight: bold">break</span>
                        now <span style="color: #666666">=</span> time<span style="color: #666666">.</span>time()
                        <span style="color: #008000; font-weight: bold">if</span> now <span style="color: #666666">-</span> last_log <span style="color: #666666">&gt;</span> <span style="color: #666666">5</span>:
                            logging<span style="color: #666666">.</span>info(
                                <span style="color: #BA2121">"LLM request in progress: </span><span style="color: #A45A77; font-weight: bold">%d</span><span style="color: #BA2121"> bytes received"</span>,
                                <span style="color: #008000">len</span>(content_bytes),
                            )
                            last_log <span style="color: #666666">=</span> now
                <span style="color: #008000; font-weight: bold">except</span> <span style="color: #CB3F38; font-weight: bold">TypeError</span>:  <span style="color: #3D7B7B; font-style: italic"># pragma: no cover - mock without iterable</span>
                    <span style="color: #008000; font-weight: bold">pass</span>

                <span style="color: #008000; font-weight: bold">if</span> content_bytes:
                    data <span style="color: #666666">=</span> json<span style="color: #666666">.</span>loads(content_bytes<span style="color: #666666">.</span>decode())
                <span style="color: #008000; font-weight: bold">else</span>:  <span style="color: #3D7B7B; font-style: italic"># pragma: no cover - fallback for mocked responses</span>
                    data <span style="color: #666666">=</span> response<span style="color: #666666">.</span>json()
                content <span style="color: #666666">=</span> data[<span style="color: #BA2121">"choices"</span>][<span style="color: #666666">0</span>][<span style="color: #BA2121">"message"</span>][<span style="color: #BA2121">"content"</span>]
                logging<span style="color: #666666">.</span>info(<span style="color: #BA2121">"LLM request completed"</span>)
                <span style="color: #008000; font-weight: bold">return</span> sanitize_summary(content)
            <span style="color: #008000; font-weight: bold">except</span> HTTPError <span style="color: #008000; font-weight: bold">as</span> exc:
                resp <span style="color: #666666">=</span> exc<span style="color: #666666">.</span>response <span style="color: #AA22FF; font-weight: bold">or</span> response
                <span style="color: #008000; font-weight: bold">try</span>:
                    err_json <span style="color: #666666">=</span> resp<span style="color: #666666">.</span>json()
                    <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">isinstance</span>(err_json, <span style="color: #008000">dict</span>):
                        error_message <span style="color: #666666">=</span> err_json<span style="color: #666666">.</span>get(<span style="color: #BA2121">"error"</span>, resp<span style="color: #666666">.</span>text)
                    <span style="color: #008000; font-weight: bold">else</span>:
                        error_message <span style="color: #666666">=</span> resp<span style="color: #666666">.</span>text
                <span style="color: #008000; font-weight: bold">except</span> (<span style="color: #CB3F38; font-weight: bold">ValueError</span>, StreamConsumedError):
                    <span style="color: #008000; font-weight: bold">try</span>:
                        error_message <span style="color: #666666">=</span> resp<span style="color: #666666">.</span>text
                    <span style="color: #008000; font-weight: bold">except</span> StreamConsumedError:
                        error_message <span style="color: #666666">=</span> <span style="color: #BA2121">""</span>
                logging<span style="color: #666666">.</span>error(<span style="color: #BA2121">"LLM request failed: </span><span style="color: #A45A77; font-weight: bold">%s</span><span style="color: #BA2121">"</span>, error_message)
                time<span style="color: #666666">.</span>sleep(<span style="color: #666666">1</span>)
            <span style="color: #008000; font-weight: bold">except</span> RequestException <span style="color: #008000; font-weight: bold">as</span> exc:
                error_message <span style="color: #666666">=</span> <span style="color: #008000">str</span>(exc)
                logging<span style="color: #666666">.</span>error(<span style="color: #BA2121">"LLM request failed: </span><span style="color: #A45A77; font-weight: bold">%s</span><span style="color: #BA2121">"</span>, error_message)
                time<span style="color: #666666">.</span>sleep(<span style="color: #666666">1</span>)

        logging<span style="color: #666666">.</span>error(<span style="color: #BA2121">"LLM request failed: </span><span style="color: #A45A77; font-weight: bold">%s</span><span style="color: #BA2121">"</span>, error_message)
        <span style="color: #008000; font-weight: bold">raise</span> <span style="color: #CB3F38; font-weight: bold">RuntimeError</span>(<span style="color: #BA2121">f"LLM request failed: </span><span style="color: #A45A77; font-weight: bold">{</span>error_message<span style="color: #A45A77; font-weight: bold">}</span><span style="color: #BA2121">"</span>)
</code></pre>
<h2>Functions</h2>
<h3 id="sanitize_summary">sanitize_summary(text: str) -&gt; str</h3>
<p>The function `sanitize_summary` processes a given text string to remove meta-commentary and unwanted content. It first checks if the text is exactly "project summary" and returns "It prints." in that case. It then removes FIM special tokens using the `strip_fim_tokens` function to prevent encoding errors. The function filters out lines that match predefined bad start phrases or contain specific unwanted substrings. It also excludes lines that begin with bullet points, or that match patterns related to the summary itself or other meta-commentary. The result is a cleaned version of the input text with such commentary removed.</p>
<pre><code><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">sanitize_summary</span>(text: <span style="color: #008000">str</span>) <span style="color: #666666">-&gt;</span> <span style="color: #008000">str</span>:
<span style="color: #bbbbbb">    </span><span style="color: #BA2121; font-style: italic">"""Return ``text`` with meta commentary removed."""</span>

    <span style="color: #008000; font-weight: bold">if</span> text<span style="color: #666666">.</span>strip() <span style="color: #666666">==</span> <span style="color: #BA2121">"project summary"</span>:
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #BA2121">"It prints."</span>

    <span style="color: #3D7B7B; font-style: italic"># Remove FIM special tokens that some models may emit. The</span>
    <span style="color: #3D7B7B; font-style: italic"># ``tiktoken`` tokenizer refuses to encode these reserved tokens and</span>
    <span style="color: #3D7B7B; font-style: italic"># raises ``DisallowedToken`` errors if they appear in the prompt. A</span>
    <span style="color: #3D7B7B; font-style: italic"># stray token can therefore crash later merging steps when we attempt</span>
    <span style="color: #3D7B7B; font-style: italic"># to re-tokenize model output. Stripping them here keeps downstream</span>
    <span style="color: #3D7B7B; font-style: italic"># processing robust.</span>
    text <span style="color: #666666">=</span> strip_fim_tokens(text)

    BAD_START_PHRASES <span style="color: #666666">=</span> [
        <span style="color: #BA2121">"summarize"</span>,
        <span style="color: #BA2121">"you are"</span>,
        <span style="color: #BA2121">"you can"</span>,
        <span style="color: #BA2121">"note that"</span>,
        <span style="color: #BA2121">"the code above"</span>,
        <span style="color: #BA2121">"this script"</span>,
        <span style="color: #BA2121">"here's how"</span>,
        <span style="color: #BA2121">"to run this"</span>,
        <span style="color: #BA2121">"let's"</span>,
        <span style="color: #BA2121">"for example"</span>,
        <span style="color: #BA2121">"you might"</span>,
        <span style="color: #BA2121">"we can"</span>,
        <span style="color: #BA2121">"should you"</span>,
        <span style="color: #BA2121">"if you want"</span>,
        <span style="color: #BA2121">"the summary"</span>,
        <span style="color: #BA2121">"this explanation"</span>,
        <span style="color: #BA2121">"this output"</span>,
        <span style="color: #BA2121">"this description"</span>,
        <span style="color: #BA2121">"this response"</span>,
    ]

    BAD_CONTAINS <span style="color: #666666">=</span> [
        <span style="color: #BA2121">"documentation engine"</span>,
        <span style="color: #BA2121">"summarize the following"</span>,
        <span style="color: #BA2121">"as an ai language model"</span>,
        <span style="color: #BA2121">"as a language model"</span>,
        <span style="color: #BA2121">"as an ai model"</span>,
        <span style="color: #BA2121">"i am an ai"</span>,
        <span style="color: #BA2121">"i'm an ai"</span>,
    ]

    lines <span style="color: #666666">=</span> text<span style="color: #666666">.</span>strip()<span style="color: #666666">.</span>splitlines()
    filtered <span style="color: #666666">=</span> []
    <span style="color: #008000; font-weight: bold">for</span> line <span style="color: #AA22FF; font-weight: bold">in</span> lines:
        stripped <span style="color: #666666">=</span> line<span style="color: #666666">.</span>strip()
        line_lower <span style="color: #666666">=</span> stripped<span style="color: #666666">.</span>lower()
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">any</span>(
            p <span style="color: #AA22FF; font-weight: bold">in</span> line_lower <span style="color: #AA22FF; font-weight: bold">or</span> (line_lower <span style="color: #AA22FF; font-weight: bold">in</span> p <span style="color: #AA22FF; font-weight: bold">and</span> <span style="color: #008000">len</span>(line_lower) <span style="color: #666666">&gt;</span> <span style="color: #666666">8</span>)
            <span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> PROMPT_LINE_SET<span style="color: #666666">.</span>union(SYSTEM_PROMPT_LINES)
        ):
            <span style="color: #008000; font-weight: bold">continue</span>
        <span style="color: #008000; font-weight: bold">if</span> stripped<span style="color: #666666">.</span>startswith(<span style="color: #BA2121">"-"</span>) <span style="color: #AA22FF; font-weight: bold">or</span> stripped<span style="color: #666666">.</span>startswith(<span style="color: #BA2121">"*"</span>):
            <span style="color: #008000; font-weight: bold">continue</span>
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">any</span>(line_lower<span style="color: #666666">.</span>startswith(p) <span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> BAD_START_PHRASES):
            <span style="color: #008000; font-weight: bold">continue</span>
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">any</span>(p <span style="color: #AA22FF; font-weight: bold">in</span> line_lower <span style="color: #008000; font-weight: bold">for</span> p <span style="color: #AA22FF; font-weight: bold">in</span> BAD_CONTAINS):
            <span style="color: #008000; font-weight: bold">continue</span>
        <span style="color: #008000; font-weight: bold">if</span> (
            <span style="color: #BA2121">"this summary"</span> <span style="color: #AA22FF; font-weight: bold">in</span> line_lower
            <span style="color: #AA22FF; font-weight: bold">or</span> <span style="color: #BA2121">"this output"</span> <span style="color: #AA22FF; font-weight: bold">in</span> line_lower
            <span style="color: #AA22FF; font-weight: bold">or</span> <span style="color: #BA2121">"this response"</span> <span style="color: #AA22FF; font-weight: bold">in</span> line_lower
            <span style="color: #AA22FF; font-weight: bold">or</span> <span style="color: #BA2121">"does not include"</span> <span style="color: #AA22FF; font-weight: bold">in</span> line_lower
            <span style="color: #AA22FF; font-weight: bold">or</span> <span style="color: #BA2121">"avoids addressing"</span> <span style="color: #AA22FF; font-weight: bold">in</span> line_lower
        ):
            <span style="color: #008000; font-weight: bold">continue</span>
        <span style="color: #008000; font-weight: bold">if</span> re<span style="color: #666666">.</span>match(<span style="color: #BA2121">r"^this (script|code|file) (does|is)\b"</span>, line_lower):
            <span style="color: #008000; font-weight: bold">continue</span>
        filtered<span style="color: #666666">.</span>append(line<span style="color: #666666">.</span>strip())

    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #BA2121">"</span><span style="color: #AA5D1F; font-weight: bold">\n</span><span style="color: #BA2121">"</span><span style="color: #666666">.</span>join(filtered)<span style="color: #666666">.</span>strip()
</code></pre>
</div>
    </main>
<script src="static/toggle.js"></script>
</body>
</html>
